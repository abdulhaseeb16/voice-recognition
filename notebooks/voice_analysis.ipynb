{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Data Analysis - Emotion Recognition\n",
    "\n",
    "This notebook demonstrates the complete pipeline for voice emotion recognition using machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "audio_data = np.load('../data/processed_audio.npy')\n",
    "labels = np.load('../data/label_names.npy')\n",
    "metadata = pd.read_csv('../data/metadata.csv')\n",
    "\n",
    "print(f\"Dataset shape: {audio_data.shape}\")\n",
    "print(f\"Number of samples: {len(labels)}\")\n",
    "print(f\"Unique emotions: {np.unique(labels)}\")\n",
    "\n",
    "# Display metadata\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "emotion_counts = pd.Series(labels).value_counts()\n",
    "emotion_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Emotions in Dataset')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Audio Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample waveforms\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "unique_emotions = np.unique(labels)\n",
    "for i, emotion in enumerate(unique_emotions[:4]):\n",
    "    # Find first sample of this emotion\n",
    "    idx = np.where(labels == emotion)[0][0]\n",
    "    \n",
    "    # Plot waveform\n",
    "    axes[i].plot(audio_data[idx])\n",
    "    axes[i].set_title(f'{emotion.title()} - Waveform')\n",
    "    axes[i].set_xlabel('Sample')\n",
    "    axes[i].set_ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spectrograms\n",
    "spectrograms = np.load('../data/spectrograms.npy')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, emotion in enumerate(unique_emotions[:4]):\n",
    "    idx = np.where(labels == emotion)[0][0]\n",
    "    \n",
    "    librosa.display.specshow(spectrograms[idx], ax=axes[i], \n",
    "                           x_axis='time', y_axis='mel', cmap='viridis')\n",
    "    axes[i].set_title(f'{emotion.title()} - Mel Spectrogram')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze features\n",
    "features_df = pd.read_csv('../data/features_summary.csv')\n",
    "\n",
    "print(f\"Feature matrix shape: {features_df.shape}\")\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "correlation_matrix = features_df.drop('label', axis=1).corr()\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distribution by emotion\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "important_features = ['mfcc_0', 'mfcc_1', 'spectral_centroid', 'zero_crossing_rate']\n",
    "\n",
    "for i, feature in enumerate(important_features):\n",
    "    for emotion in unique_emotions:\n",
    "        emotion_data = features_df[features_df['label'] == np.where(unique_emotions == emotion)[0][0]]\n",
    "        axes[i].hist(emotion_data[feature], alpha=0.7, label=emotion, bins=10)\n",
    "    \n",
    "    axes[i].set_title(f'{feature} Distribution by Emotion')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display model results\n",
    "try:\n",
    "    results_df = pd.read_csv('../results/detailed_results.csv')\n",
    "    print(\"Model Performance Comparison:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(results_df['Model'], results_df['Accuracy'], \n",
    "                   color=['skyblue', 'lightgreen', 'salmon', 'gold', 'lightcoral'])\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for bar, acc in zip(bars, results_df['Accuracy']):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Results not found. Run the training and evaluation scripts first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions and Insights\n",
    "\n",
    "### Key Findings:\n",
    "1. **Feature Importance**: MFCC features show strong discriminative power for emotion classification\n",
    "2. **Model Performance**: Deep learning models (CNN/RNN) generally outperform classical ML approaches\n",
    "3. **Spectral Features**: Spectral centroid and zero-crossing rate provide valuable emotion-specific information\n",
    "\n",
    "### Technical Achievements:\n",
    "- Implemented complete audio preprocessing pipeline\n",
    "- Extracted multiple types of audio features (MFCC, Chroma, Spectral)\n",
    "- Compared classical ML vs deep learning approaches\n",
    "- Created interactive web application for real-time prediction\n",
    "\n",
    "### Future Improvements:\n",
    "- Experiment with larger datasets (RAVDESS, IEMOCAP)\n",
    "- Implement attention mechanisms in neural networks\n",
    "- Add real-time audio recording capability\n",
    "- Explore transfer learning with pre-trained audio models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}